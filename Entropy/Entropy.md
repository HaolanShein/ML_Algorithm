# 熵
物体内部的混乱程度（一件事发生的不确定性）

$$H(X)=-\sum_{x\in X}{P(x)log(P(x))}$$

所有的概率值都是0-1之间，那么最终的H(X)必然也是一个正数。

## 熵值的大小意味着什么呢？

举个例子：杂货市场和苹果专卖店。比如说杂货市场中有100件商品，那么可以选择商品就有很多，因此购买其中一件商品的概率值就很小了。对于熵的公式来说，概率值越小，熵值就也越大。

## 熵是如何应用在分类任务中呢？

想象一个分类任务，我们希望得到的结果是什么样的呢？

$$A[1,1,1,1,1,1],B[1,2,3,4,5,3]$$

显然A集合才是我们希望得到的结果，因为它的熵值比较小。因此，熵值可以作为**衡量分类质量**的指标了。

比如说银行贷款，有两个指标，分别是年龄和工资。然后我想看一下这两个指标哪一个对结果也就是贷款的数目影响大。

当我用年龄对贷款人进行分组时，所得到的结果熵值为A，用工资分组时，所得到结果的熵值为B。

如果说B小于A，我就可以认为工资这个特征要比年龄更重要一些。

# 激活函数

## Sigmoid函数

![avatar](/Entropy/SigmoidFunction.png)

Sigmoid是常用的非线性的激活函数

能够把连续值压缩到0-1区间上，就可以把任何一个输出变成概率值

缺点：杀死梯度，非原点中心对称

非原点对称就会使得输出值全为整数，会导致梯度全为正数或全为负数；优化更新会产生阶梯式情况

## Tanh函数

![avatar](/Entropy/TanhFunciton.png)

原点中心对称，输出在-1到1之间

梯度消失现象依然存在

## Relu函数

![avatar](/Entropy/ReluFunction.png)

$$f(x)=max(0,x)$$

公式简单实用

解决了梯度消失现象，收敛速度更快

## Leaky Relu函数

$$f(x)=max(0.01x,x)$$

解决了Relu函数会杀死一部分神经元的情况
