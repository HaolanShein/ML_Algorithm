# Regression

相关分析是研究两个或两个以上的变量之间相关程度及大小的一种统计方法

回归分析是寻找存在相关关系的变量间的数学表达式，并进行统计推断的一种统计方法

在对回归分析进行分类时，主要有两种分类方法：

*  根据变量的数目，可以分类一元回归、多元回归

*  根据自变量与因变量的表现形式，分为线性与非线性

所以，回归分析包括四个方向：一元线性回归分析、多元线性回归分析、一元非线性回归分析、多元非线性回归分析

## 回归分析的一般步骤

*  确定回归方程中解释变量和被解释变量

*  确定回归模型，建立回归方程

*  对回归方程进行各种检验

*  利用回归方程进行预测

## 一元线性回归分析

*  因变量（dependent variable）：被预测或被解释的变量，用y表示

*  自变量（independent variable）：预测或解释因变量的变量，用x表示

*  对于具有线性关系的两个变量，可以用一个方程来表示它们之间的线性关系

*  描述因变量y如何依赖于自变量x和误差项$\epsilon$的方程称为回归模型。对于只涉及一个自变量的一元线性回归模型可表示为：

$$y = \beta _0 + \beta _1x + \epsilon$$

*  y叫做因变量或被解释变量
*  x叫做自变量或解释变量
*  $\beta_0$表示截距
*  $\beta_1$表示斜率
*  $\epsilon$表示误差项，反映除x和y之间的线性关系外的随机因素对y的影响

## 回归方程

描述因变量y的期望值如何依赖于自变量x的方程称为回归方程。根据对一元线性回归模型的假设，可以得到它的回归方程：

$$E(y) = \beta_0 + \beta_1x$$

*  如果回归方程中的参数已知，对于一个给定的x值，利用回归方程就能计算出y的期望值
*  用样本统计量代替回归方程中的未知参数，就得到估计的回归方程，简称回归直线

## 参数的最小二乘法估计

对于回归直线，关键在于求解参数，常用高斯提出的最小二乘法，它是使因变量的观察量y与估计值之间的离差平方和达到最小来求解

## 误差

误差$\epsilon$是独立并且具有相同的分布，并且服从均值为0方差为$\theta^2$的**高斯分布**

![avatar](/Regression/NormalDistribution.png)

### 以银行贷款举例

*  独立：张三和李四一起来贷款，他们俩没关系
*  同分布：他们俩都得来同一间银行
*  高斯分布：银行可能会多给，也可能会少给，但是绝大多数情况下，这个浮动不会太大，极小情况下浮动会比较大
